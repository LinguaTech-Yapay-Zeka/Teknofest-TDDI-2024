{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfIfHCm-w4QX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.regularizers import L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTZ4dkDDw7Ad"
      },
      "outputs": [],
      "source": [
        "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "df = pd.read_csv(\"hf://datasets/winvoker/turkish-sentiment-analysis-dataset/\" + splits[\"train\"])\n",
        "df = df.drop([\"dataset\"] , axis=1)\n",
        "df_Positive = df[df['label']=='Positive']\n",
        "df_Negative = df[df['label']== 'Negative']\n",
        "df_Notr = df[df['label']=='Notr']\n",
        "df_Notr_az = df_Notr.sample(50000)\n",
        "df_Positive_az = df_Positive.sample(50000)\n",
        "df_Negative_az = df_Negative.sample(50000)\n",
        "df = pd.concat([df_Positive_az, df_Notr_az, df_Negative_az])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUaau9V-xTNJ",
        "outputId": "ee0c2aea-966e-4119-f637-63a0b64a953d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "GPU memory growth enabled\n",
            "Epoch 1/3\n",
            "15000/15000 [==============================] - 243s 16ms/step - loss: 0.2309 - accuracy: 0.9135 - val_loss: 0.4171 - val_accuracy: 0.8232\n",
            "Epoch 2/3\n",
            "15000/15000 [==============================] - 213s 14ms/step - loss: 0.0992 - accuracy: 0.9655 - val_loss: 0.5504 - val_accuracy: 0.7954\n",
            "Epoch 3/3\n",
            "15000/15000 [==============================] - 223s 15ms/step - loss: 0.0448 - accuracy: 0.9861 - val_loss: 0.5367 - val_accuracy: 0.8122\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "sentiments = df['label'].tolist()\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100 \n",
        "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=max_length)\n",
        "\n",
        "sentiment_map = {'Positive': 0, 'Notr': 1, 'Negative': 2}\n",
        "y = [sentiment_map[sentiment] for sentiment in sentiments]\n",
        "y = to_categorical(y)\n",
        "\n",
        "X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "with tf.device('/GPU:0'): \n",
        "    inputs = Input(shape=(max_length,))\n",
        "    x = Embedding(vocab_size, 128)(inputs)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "with tf.device('/GPU:0'): \n",
        "    history = model.fit(X, y, epochs=3, batch_size=8, validation_split=0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gHzKtUfxgkx",
        "outputId": "b36727a3-776b-423a-a28b-f2f28ff582ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "GPU memory growth enabled\n",
            "Epoch 1/3\n",
            "15000/15000 [==============================] - 252s 17ms/step - loss: 0.2864 - accuracy: 0.9056 - val_loss: 0.5891 - val_accuracy: 0.7764 - lr: 0.0010\n",
            "Epoch 2/3\n",
            "15000/15000 [==============================] - 214s 14ms/step - loss: 0.1352 - accuracy: 0.9610 - val_loss: 0.5101 - val_accuracy: 0.8236 - lr: 0.0010\n",
            "Epoch 3/3\n",
            "15000/15000 [==============================] - 214s 14ms/step - loss: 0.0807 - accuracy: 0.9804 - val_loss: 0.7766 - val_accuracy: 0.7981 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "sentiments = df['label'].tolist()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=max_length)\n",
        "\n",
        "sentiment_map = {'Positive': 0, 'Notr': 1, 'Negative': 2}\n",
        "y = [sentiment_map[sentiment] for sentiment in sentiments]\n",
        "y = to_categorical(y)\n",
        "\n",
        "X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "with tf.device('/GPU:0'): \n",
        "    inputs = Input(shape=(max_length,))\n",
        "    x = Embedding(vocab_size, 128)(inputs)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = Dense(64, activation='relu', kernel_regularizer=L2(0.01))(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=10,\n",
        "        batch_size=8,\n",
        "        validation_split=0.2,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_scheduler, early_stopping]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNmNVZOt1i53",
        "outputId": "2380ad70-5aec-4a0b-9313-750c0dafa67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "GPU memory growth enabled\n",
            "Epoch 1/10\n",
            "15000/15000 [==============================] - 185s 12ms/step - loss: 0.4688 - accuracy: 0.8419 - val_loss: 0.9225 - val_accuracy: 0.6320 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "15000/15000 [==============================] - 156s 10ms/step - loss: 0.2743 - accuracy: 0.9163 - val_loss: 0.5710 - val_accuracy: 0.7977 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "15000/15000 [==============================] - 156s 10ms/step - loss: 0.2240 - accuracy: 0.9334 - val_loss: 0.7536 - val_accuracy: 0.7485 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "15000/15000 [==============================] - 160s 11ms/step - loss: 0.1984 - accuracy: 0.9438 - val_loss: 0.4088 - val_accuracy: 0.8800 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "15000/15000 [==============================] - 160s 11ms/step - loss: 0.1844 - accuracy: 0.9483 - val_loss: 0.5803 - val_accuracy: 0.8332 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "15000/15000 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9529\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "15000/15000 [==============================] - 156s 10ms/step - loss: 0.1680 - accuracy: 0.9529 - val_loss: 0.7098 - val_accuracy: 0.8052 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "14999/15000 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9595Restoring model weights from the end of the best epoch: 4.\n",
            "15000/15000 [==============================] - 155s 10ms/step - loss: 0.1472 - accuracy: 0.9595 - val_loss: 0.6783 - val_accuracy: 0.8150 - lr: 5.0000e-04\n",
            "Epoch 7: early stopping\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "sentiments = df['label'].tolist()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100 \n",
        "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=max_length)\n",
        "\n",
        "sentiment_map = {'Positive': 0, 'Notr': 1, 'Negative': 2}\n",
        "y = [sentiment_map[sentiment] for sentiment in sentiments]\n",
        "y = to_categorical(y)\n",
        "\n",
        "X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    inputs = Input(shape=(max_length,))\n",
        "    x = Embedding(vocab_size, 128)(inputs)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(32, activation='relu', kernel_regularizer=L2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "initial_learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    history = model.fit(\n",
        "        X, y,\n",
        "        epochs=10,\n",
        "        batch_size=8,\n",
        "        validation_split=0.2,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_scheduler, early_stopping]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oEWh22fGILpT",
        "outputId": "64db5136-c1e2-4c27-a598-3387f5700576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'nötr'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Turkcell normal\"\n",
        "predict_sentiment(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT_eLDEWIPb4"
      },
      "outputs": [],
      "source": [
        "model.save('sentimentmodel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp1w3Ox9IiOr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8UCSGasNkPj"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEJGso5XUuUv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
